{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jsonld-ex Reproducible Benchmark Suite\n",
    "\n",
    "**Purpose:** Run the complete jsonld-ex benchmark suite on this system and save\n",
    "timestamped, system-fingerprinted results for cross-platform comparison.\n",
    "\n",
    "**For paper:** *jsonld-ex: Backward-Compatible JSON-LD Extensions for AI/ML Data Exchange*\n",
    "\n",
    "**Methodology:**\n",
    "- 30 trials per measurement, 3 warmup iterations\n",
    "- 95% confidence intervals via t-distribution\n",
    "- Fixed random seed (42) for deterministic data generation\n",
    "- System fingerprint captured for hardware-aware comparison\n",
    "\n",
    "**What should be invariant across systems:**\n",
    "- Byte ratios (PROV-O verbosity, SHACL verbosity, IoT payload savings)\n",
    "- Calibration metrics (ECE, Brier score) — deterministic with fixed seed\n",
    "- Information richness results — algebraic identity\n",
    "- Trust ≡ scalar equivalence — numerical identity within 1e-12\n",
    "- Scaling *shape* (linear, sublinear) for fusion, trust chains, merge\n",
    "\n",
    "**What will vary (expected):**\n",
    "- Absolute throughput (ops/sec, μs/op) — hardware-dependent\n",
    "- Speedup ratios vs baselines — may shift ±20% across CPUs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. System Fingerprint\n",
    "\n",
    "Captures comprehensive hardware/software info for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "import sys\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "def get_system_fingerprint() -> dict:\n",
    "    \"\"\"Collect comprehensive system information.\"\"\"\n",
    "    info = {\n",
    "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"platform\": {\n",
    "            \"system\": platform.system(),\n",
    "            \"release\": platform.release(),\n",
    "            \"version\": platform.version(),\n",
    "            \"machine\": platform.machine(),\n",
    "            \"processor\": platform.processor(),\n",
    "            \"python_version\": sys.version,\n",
    "            \"python_implementation\": platform.python_implementation(),\n",
    "        },\n",
    "        \"cpu\": {},\n",
    "        \"memory\": {},\n",
    "        \"gpu\": {},\n",
    "        \"environment\": {},\n",
    "    }\n",
    "\n",
    "    # CPU details\n",
    "    try:\n",
    "        import multiprocessing\n",
    "        info[\"cpu\"][\"logical_cores\"] = multiprocessing.cpu_count()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Linux-specific CPU info\n",
    "    if platform.system() == \"Linux\":\n",
    "        try:\n",
    "            with open(\"/proc/cpuinfo\") as f:\n",
    "                cpuinfo = f.read()\n",
    "            for line in cpuinfo.split(\"\\n\"):\n",
    "                if \"model name\" in line:\n",
    "                    info[\"cpu\"][\"model\"] = line.split(\":\")[1].strip()\n",
    "                    break\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            with open(\"/proc/meminfo\") as f:\n",
    "                for line in f:\n",
    "                    if \"MemTotal\" in line:\n",
    "                        kb = int(line.split()[1])\n",
    "                        info[\"memory\"][\"total_gb\"] = round(kb / 1024 / 1024, 1)\n",
    "                        break\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Windows-specific CPU info\n",
    "    elif platform.system() == \"Windows\":\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"wmic\", \"cpu\", \"get\", \"Name\", \"/value\"],\n",
    "                capture_output=True, text=True, timeout=5\n",
    "            )\n",
    "            for line in result.stdout.split(\"\\n\"):\n",
    "                if \"Name=\" in line:\n",
    "                    info[\"cpu\"][\"model\"] = line.split(\"=\")[1].strip()\n",
    "        except Exception:\n",
    "            info[\"cpu\"][\"model\"] = platform.processor()\n",
    "\n",
    "    # macOS-specific CPU info\n",
    "    elif platform.system() == \"Darwin\":\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"sysctl\", \"-n\", \"machdep.cpu.brand_string\"],\n",
    "                capture_output=True, text=True, timeout=5\n",
    "            )\n",
    "            info[\"cpu\"][\"model\"] = result.stdout.strip()\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"sysctl\", \"-n\", \"hw.memsize\"],\n",
    "                capture_output=True, text=True, timeout=5\n",
    "            )\n",
    "            info[\"memory\"][\"total_gb\"] = round(int(result.stdout.strip()) / 1024**3, 1)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # GPU detection\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"nvidia-smi\", \"--query-gpu=name,memory.total,driver_version\",\n",
    "             \"--format=csv,noheader,nounits\"],\n",
    "            capture_output=True, text=True, timeout=10\n",
    "        )\n",
    "        if result.returncode == 0 and result.stdout.strip():\n",
    "            parts = result.stdout.strip().split(\", \")\n",
    "            info[\"gpu\"][\"name\"] = parts[0] if len(parts) > 0 else \"unknown\"\n",
    "            info[\"gpu\"][\"memory_mb\"] = int(parts[1]) if len(parts) > 1 else 0\n",
    "            info[\"gpu\"][\"driver\"] = parts[2] if len(parts) > 2 else \"unknown\"\n",
    "    except (FileNotFoundError, Exception):\n",
    "        info[\"gpu\"][\"available\"] = False\n",
    "\n",
    "    # Detect Colab / cloud environment\n",
    "    if \"COLAB_RELEASE_TAG\" in os.environ:\n",
    "        info[\"environment\"][\"type\"] = \"google_colab\"\n",
    "        info[\"environment\"][\"colab_tag\"] = os.environ.get(\"COLAB_RELEASE_TAG\", \"\")\n",
    "    elif \"CODESPACES\" in os.environ:\n",
    "        info[\"environment\"][\"type\"] = \"github_codespaces\"\n",
    "    elif os.path.exists(\"/proc/1/cgroup\"):\n",
    "        try:\n",
    "            with open(\"/proc/1/cgroup\") as f:\n",
    "                if \"docker\" in f.read():\n",
    "                    info[\"environment\"][\"type\"] = \"docker\"\n",
    "        except Exception:\n",
    "            pass\n",
    "    if \"type\" not in info[\"environment\"]:\n",
    "        info[\"environment\"][\"type\"] = \"local\"\n",
    "\n",
    "    return info\n",
    "\n",
    "\n",
    "fingerprint = get_system_fingerprint()\n",
    "print(json.dumps(fingerprint, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install from source (ensures benchmarks match the exact code version)\n",
    "# PyPI may lag behind - source install guarantees consistency\n",
    "!pip install -q \"jsonld-ex[iot,bench] @ git+https://github.com/jemsbhai/jsonld-ex.git#subdirectory=packages/python\" cbor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "import jsonld_ex\n",
    "print(f\"jsonld-ex version: {jsonld_ex.__version__}\")\n",
    "\n",
    "# Quick smoke test\n",
    "from jsonld_ex.confidence_algebra import Opinion, cumulative_fuse\n",
    "o1 = Opinion(belief=0.7, disbelief=0.1, uncertainty=0.2)\n",
    "o2 = Opinion(belief=0.5, disbelief=0.3, uncertainty=0.2)\n",
    "fused = cumulative_fuse(o1, o2)\n",
    "print(f\"Smoke test: fuse({o1.belief:.1f}, {o2.belief:.1f}) -> belief={fused.belief:.4f} ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repo for benchmark scripts\n",
    "# (benchmark runner scripts are not part of the PyPI package)\n",
    "!git clone --depth 1 https://github.com/jemsbhai/jsonld-ex.git /tmp/jsonld-ex-repo 2>/dev/null || \\\n",
    "    (cd /tmp/jsonld-ex-repo && git pull --ff-only)\n",
    "\n",
    "import os\n",
    "os.chdir(\"/tmp/jsonld-ex-repo/benchmarks\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Benchmark files: {sorted(f for f in os.listdir('.') if f.startswith('bench_'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Full Benchmark Suite\n",
    "\n",
    "This takes **5-15 minutes** depending on hardware. All 6 domains + baselines,\n",
    "30 trials each with 3 warmup iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Ensure the benchmark directory and package source are on the path\n",
    "bench_dir = \"/tmp/jsonld-ex-repo/benchmarks\"\n",
    "src_dir = \"/tmp/jsonld-ex-repo/packages/python/src\"\n",
    "if bench_dir not in sys.path:\n",
    "    sys.path.insert(0, bench_dir)\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.insert(0, src_dir)\n",
    "\n",
    "os.chdir(bench_dir)\n",
    "\n",
    "# Run each domain individually for better progress visibility\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "overall_start = time.perf_counter()\n",
    "all_results = {}\n",
    "\n",
    "print(\"Running Domain 1: OWL/RDF Ecosystem Interoperability...\")\n",
    "import bench_owl_rdf\n",
    "d1 = bench_owl_rdf.run_all()\n",
    "all_results[\"domain_1_owl_rdf\"] = {\n",
    "    \"prov_o_verbosity\": d1.prov_o_verbosity,\n",
    "    \"shacl_verbosity\": d1.shacl_verbosity,\n",
    "    \"round_trip_fidelity\": d1.round_trip_fidelity,\n",
    "    \"conversion_throughput\": d1.conversion_throughput,\n",
    "}\n",
    "print(f\"  ✓ Domain 1 complete ({time.perf_counter() - overall_start:.1f}s elapsed)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running Domain 2: Multi-Agent KG Construction...\")\n",
    "import bench_multi_agent\n",
    "d2 = bench_multi_agent.run_all()\n",
    "all_results[\"domain_2_multi_agent\"] = {\n",
    "    \"merge_throughput\": d2.merge_throughput,\n",
    "    \"merge_by_conflict_rate\": d2.merge_by_conflict_rate,\n",
    "    \"propagation_overhead\": d2.propagation_overhead,\n",
    "    \"combination_comparison\": d2.combination_comparison,\n",
    "    \"diff_throughput\": d2.diff_throughput,\n",
    "}\n",
    "print(f\"  ✓ Domain 2 complete ({time.perf_counter() - overall_start:.1f}s elapsed)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running Domain 3: Healthcare IoT Pipeline...\")\n",
    "import bench_iot\n",
    "d3 = bench_iot.run_all()\n",
    "all_results[\"domain_3_iot\"] = {\n",
    "    \"payload_sizes\": d3.payload_sizes,\n",
    "    \"pipeline_throughput\": d3.pipeline_throughput,\n",
    "    \"mqtt_overhead\": d3.mqtt_overhead,\n",
    "    \"batch_scaling\": d3.batch_scaling,\n",
    "}\n",
    "print(f\"  ✓ Domain 3 complete ({time.perf_counter() - overall_start:.1f}s elapsed)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running Domain 4: RAG Pipeline & Temporal Queries...\")\n",
    "import bench_rag\n",
    "d4 = bench_rag.run_all()\n",
    "all_results[\"domain_4_rag\"] = {\n",
    "    \"confidence_filter\": d4.confidence_filter,\n",
    "    \"temporal_query\": d4.temporal_query,\n",
    "    \"temporal_diff\": d4.temporal_diff_bench,\n",
    "    \"rag_pipeline\": d4.rag_pipeline,\n",
    "}\n",
    "print(f\"  ✓ Domain 4 complete ({time.perf_counter() - overall_start:.1f}s elapsed)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running Baselines: rdflib, pyshacl comparisons...\")\n",
    "import bench_baselines\n",
    "db = bench_baselines.run_all()\n",
    "all_results[\"baselines\"] = {\n",
    "    \"prov_o_construction\": db.prov_o_construction,\n",
    "    \"shacl_validation\": db.shacl_validation,\n",
    "    \"graph_merge\": db.graph_merge,\n",
    "    \"temporal_query\": db.temporal_query,\n",
    "}\n",
    "print(f\"  ✓ Baselines complete ({time.perf_counter() - overall_start:.1f}s elapsed)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running Domain 5: Confidence Algebra (Subjective Logic)...\")\n",
    "import bench_algebra\n",
    "d5 = bench_algebra.run_all()\n",
    "all_results[\"domain_5_confidence_algebra\"] = {\n",
    "    \"cumulative_fusion\": d5.cumulative_fusion,\n",
    "    \"averaging_fusion\": d5.averaging_fusion,\n",
    "    \"trust_discount_chain\": d5.trust_discount_chain,\n",
    "    \"trust_vs_scalar\": d5.trust_vs_scalar,\n",
    "    \"deduction\": d5.deduction,\n",
    "    \"temporal_decay\": d5.temporal_decay,\n",
    "    \"opinion_formation\": d5.opinion_formation,\n",
    "    \"information_richness\": d5.information_richness,\n",
    "    \"calibration\": d5.calibration,\n",
    "}\n",
    "print(f\"  ✓ Domain 5 complete ({time.perf_counter() - overall_start:.1f}s elapsed)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running Domain 6: Neuro-Symbolic Bridge Pipeline...\")\n",
    "import bench_bridge\n",
    "d6 = bench_bridge.run_all()\n",
    "all_results[\"domain_6_neuro_symbolic_bridge\"] = {\n",
    "    \"pipeline_comparison\": d6.pipeline_comparison,\n",
    "    \"metadata_richness\": d6.metadata_richness,\n",
    "}\n",
    "\n",
    "total_sec = time.perf_counter() - overall_start\n",
    "print(f\"  ✓ Domain 6 complete\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ALL BENCHMARKS COMPLETE in {total_sec:.1f}s\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Assemble & Save Results\n",
    "\n",
    "Results are saved with:\n",
    "- **Timestamp** — when this run occurred\n",
    "- **System fingerprint** — exact hardware/software description\n",
    "- **Package version** — jsonld-ex version under test\n",
    "- **Git commit** — exact source code version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Get git commit hash for exact reproducibility\n",
    "try:\n",
    "    git_hash = subprocess.run(\n",
    "        [\"git\", \"rev-parse\", \"HEAD\"],\n",
    "        capture_output=True, text=True, timeout=5,\n",
    "        cwd=\"/tmp/jsonld-ex-repo\"\n",
    "    ).stdout.strip()\n",
    "except Exception:\n",
    "    git_hash = \"unknown\"\n",
    "\n",
    "try:\n",
    "    import jsonld_ex\n",
    "    pkg_version = jsonld_ex.__version__\n",
    "except Exception:\n",
    "    pkg_version = \"unknown\"\n",
    "\n",
    "# Build system label for filenames\n",
    "cpu_short = fingerprint.get(\"cpu\", {}).get(\"model\", \"unknown-cpu\")\n",
    "# Sanitize for filename: keep only alphanumeric, dash, underscore\n",
    "cpu_label = \"\".join(c if c.isalnum() or c in \"-_\" else \"-\" for c in cpu_short)[:40]\n",
    "env_type = fingerprint.get(\"environment\", {}).get(\"type\", \"local\")\n",
    "\n",
    "ts = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H-%M-%SZ\")\n",
    "system_label = f\"{env_type}_{cpu_label}\"\n",
    "\n",
    "# Assemble final results\n",
    "final_results = {\n",
    "    \"metadata\": {\n",
    "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"total_seconds\": round(total_sec, 2),\n",
    "        \"jsonld_ex_version\": pkg_version,\n",
    "        \"git_commit\": git_hash,\n",
    "        \"python_version\": sys.version.split()[0],\n",
    "        \"system_label\": system_label,\n",
    "    },\n",
    "    \"system_fingerprint\": fingerprint,\n",
    "    **all_results,\n",
    "}\n",
    "\n",
    "# Save with timestamped + system-labeled filename\n",
    "out_dir = \"/tmp/jsonld-ex-repo/benchmarks/results\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "filename = f\"benchmark_results_{ts}_{system_label}.json\"\n",
    "filepath = os.path.join(out_dir, filename)\n",
    "\n",
    "with open(filepath, \"w\") as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {filepath}\")\n",
    "print(f\"File size: {os.path.getsize(filepath) / 1024:.1f} KB\")\n",
    "print(f\"System label: {system_label}\")\n",
    "print(f\"Git commit: {git_hash[:12]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Invariant Verification\n",
    "\n",
    "These values **must** be identical across all systems (deterministic, fixed seed).\n",
    "Any deviation indicates a bug or version mismatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CROSS-SYSTEM INVARIANT CHECKS\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "checks_passed = 0\n",
    "checks_failed = 0\n",
    "\n",
    "def check(name, actual, expected, tolerance=0.0):\n",
    "    global checks_passed, checks_failed\n",
    "    if isinstance(expected, bool):\n",
    "        ok = actual == expected\n",
    "    elif tolerance > 0:\n",
    "        ok = abs(actual - expected) <= tolerance\n",
    "    else:\n",
    "        ok = actual == expected\n",
    "    status = \"✓ PASS\" if ok else \"✗ FAIL\"\n",
    "    if ok:\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        checks_failed += 1\n",
    "    print(f\"  {status}: {name} = {actual} (expected {expected})\")\n",
    "\n",
    "# 1. Trust ≡ scalar equivalence (must all be True)\n",
    "print(\"\\n--- Trust ≡ Scalar Equivalence ---\")\n",
    "for k, v in d5.trust_vs_scalar.items():\n",
    "    check(f\"trust_vs_scalar[{k}].equivalent\", v[\"numerically_equivalent\"], True)\n",
    "\n",
    "# 2. Information richness (all pairs must have same scalar)\n",
    "print(\"\\n--- Information Richness (same scalar) ---\")\n",
    "for k, v in d5.information_richness.items():\n",
    "    check(f\"info_richness[{k}].same_scalar\", v[\"same_scalar\"], True)\n",
    "\n",
    "# 3. Calibration ECE and Brier (deterministic with seed 42)\n",
    "print(\"\\n--- Calibration Metrics (deterministic) ---\")\n",
    "check(\"ECE\", d5.calibration[\"expected_calibration_error\"], 0.0340, tolerance=0.0001)\n",
    "check(\"Brier\", d5.calibration[\"brier_score\"], 0.1880, tolerance=0.0001)\n",
    "\n",
    "# 4. Byte ratios (deterministic — same data, same serialization)\n",
    "print(\"\\n--- Byte Ratios (deterministic) ---\")\n",
    "for k, v in d1.prov_o_verbosity.items():\n",
    "    check(f\"prov_o_byte_ratio[{k}]\", v[\"byte_ratio\"], v[\"byte_ratio\"])  # self-check structure\n",
    "\n",
    "# 5. Round-trip fidelity\n",
    "print(\"\\n--- Round-trip Fidelity ---\")\n",
    "check(\"round_trip_fidelity\", d1.round_trip_fidelity[\"fidelity\"], 1.0)\n",
    "\n",
    "# 6. Validation pass rate in bridge pipeline\n",
    "print(\"\\n--- Bridge Pipeline Validation ---\")\n",
    "if \"n=100\" in d6.pipeline_comparison:\n",
    "    bridge_100 = d6.pipeline_comparison[\"n=100\"]\n",
    "    check(\"bridge_invalid_nodes\", bridge_100[\"jsonld_ex\"][\"metadata\"][\"invalid_nodes\"], 0)\n",
    "\n",
    "# 7. Metadata dimensions\n",
    "print(\"\\n--- Metadata Richness ---\")\n",
    "check(\"jsonld_ex_dimensions\", d6.metadata_richness[\"jsonld_ex_preserves\"][\"metadata_dimensions\"], 10)\n",
    "check(\"adhoc_dimensions\", d6.metadata_richness[\"adhoc_preserves\"][\"metadata_dimensions\"], 2)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"INVARIANT CHECKS: {checks_passed} passed, {checks_failed} failed\")\n",
    "if checks_failed > 0:\n",
    "    print(\"⚠ FAILURES detected — investigate version mismatch or bug\")\n",
    "else:\n",
    "    print(\"✓ All invariants hold — results are cross-system consistent\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(f\"KEY RESULTS — {system_label}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Algebra\n",
    "print(\"\\n--- Confidence Algebra ---\")\n",
    "cf2 = d5.cumulative_fusion[\"n=2\"]\n",
    "print(f\"  Binary cumulative fusion: {cf2['mean_us']:.2f} μs ({cf2['ops_per_sec']:,.0f} ops/sec)\")\n",
    "af100 = d5.averaging_fusion[\"n=100\"]\n",
    "print(f\"  100-way averaging fusion: {af100['mean_us']:.2f} μs ({af100['ops_per_sec']:,.0f} ops/sec)\")\n",
    "print(f\"  Calibration ECE: {d5.calibration['expected_calibration_error']:.4f}\")\n",
    "print(f\"  Calibration Brier: {d5.calibration['brier_score']:.4f}\")\n",
    "\n",
    "# Baselines\n",
    "print(\"\\n--- Baseline Speedups ---\")\n",
    "for k, v in db.prov_o_construction.items():\n",
    "    print(f\"  PROV-O [{k}]: {v['speedup']}x faster than rdflib\")\n",
    "for k, v in db.graph_merge.items():\n",
    "    print(f\"  Merge [{k}]: {v['speedup']}x faster than rdflib+SPARQL\")\n",
    "\n",
    "# Bridge\n",
    "print(\"\\n--- Neuro-Symbolic Bridge ---\")\n",
    "for k, v in d6.pipeline_comparison.items():\n",
    "    jl = v['jsonld_ex']\n",
    "    print(f\"  {k}: {jl['mean_sec']*1000:.1f}ms \"\n",
    "          f\"({jl['nodes_per_sec']:,.0f} nodes/sec, \"\n",
    "          f\"{v['overhead_factor']}x vs ad-hoc, \"\n",
    "          f\"10 vs 2 metadata dims)\")\n",
    "\n",
    "# IoT\n",
    "print(\"\\n--- IoT Payload ---\")\n",
    "for k, v in d3.payload_sizes.items():\n",
    "    print(f\"  {k}: {v['savings_pct']}% reduction (JSON {v['json_bytes']:,}B → gzip+CBOR {v['gzip_cbor_bytes']:,}B)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Download Results\n",
    "\n",
    "Download the JSON results file for cross-system comparison.\n",
    "\n",
    "**Collaborators:** Please share your results file so we can compile\n",
    "the cross-system comparison table for the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab: auto-download\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(filepath)\n",
    "    print(f\"Downloaded: {filename}\")\n",
    "except ImportError:\n",
    "    # Not in Colab — just print the path\n",
    "    print(f\"Results saved at: {filepath}\")\n",
    "    print(f\"Copy this file and share it for cross-system comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cross-System Comparison (Optional)\n",
    "\n",
    "If you have results from multiple systems, place them all in the\n",
    "`benchmarks/results/` directory and run this cell to generate a\n",
    "comparison table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "result_files = sorted(glob.glob(os.path.join(out_dir, \"benchmark_results_2*.json\")))\n",
    "print(f\"Found {len(result_files)} result file(s):\\n\")\n",
    "\n",
    "if len(result_files) >= 2:\n",
    "    comparison_rows = []\n",
    "    for rf in result_files:\n",
    "        with open(rf) as f:\n",
    "            data = json.load(f)\n",
    "        meta = data.get(\"metadata\", {})\n",
    "        fp = data.get(\"system_fingerprint\", {})\n",
    "        cpu = fp.get(\"cpu\", {}).get(\"model\", \"unknown\")\n",
    "        env = fp.get(\"environment\", {}).get(\"type\", \"unknown\")\n",
    "        cores = fp.get(\"cpu\", {}).get(\"logical_cores\", \"?\")\n",
    "        ram = fp.get(\"memory\", {}).get(\"total_gb\", \"?\")\n",
    "\n",
    "        # Extract key metrics\n",
    "        d5_data = data.get(\"domain_5_confidence_algebra\", {})\n",
    "        cum_n2 = d5_data.get(\"cumulative_fusion\", {}).get(\"n=2\", {})\n",
    "        cal = d5_data.get(\"calibration\", {})\n",
    "\n",
    "        baselines = data.get(\"baselines\", {})\n",
    "        prov_speedups = []\n",
    "        for k, v in baselines.get(\"prov_o_construction\", {}).items():\n",
    "            prov_speedups.append(v.get(\"speedup\", 0))\n",
    "\n",
    "        bridge = data.get(\"domain_6_neuro_symbolic_bridge\", {}).get(\"pipeline_comparison\", {})\n",
    "        bridge_1k = bridge.get(\"n=1000\", {})\n",
    "        jl_1k = bridge_1k.get(\"jsonld_ex\", {})\n",
    "\n",
    "        row = {\n",
    "            \"file\": os.path.basename(rf),\n",
    "            \"env\": env,\n",
    "            \"cpu\": cpu[:30],\n",
    "            \"cores\": cores,\n",
    "            \"ram_gb\": ram,\n",
    "            \"fusion_us\": cum_n2.get(\"mean_us\", \"?\"),\n",
    "            \"fusion_ops\": cum_n2.get(\"ops_per_sec\", \"?\"),\n",
    "            \"ECE\": cal.get(\"expected_calibration_error\", \"?\"),\n",
    "            \"brier\": cal.get(\"brier_score\", \"?\"),\n",
    "            \"prov_speedup_avg\": round(sum(prov_speedups)/len(prov_speedups), 1) if prov_speedups else \"?\",\n",
    "            \"bridge_1k_ms\": round(jl_1k.get(\"mean_sec\", 0)*1000, 1) if jl_1k else \"?\",\n",
    "            \"bridge_1k_nps\": jl_1k.get(\"nodes_per_sec\", \"?\"),\n",
    "        }\n",
    "        comparison_rows.append(row)\n",
    "\n",
    "    # Print comparison table\n",
    "    print(f\"{'System':<35} {'Cores':<6} {'RAM':<6} {'Fusion(μs)':<11} \"\n",
    "          f\"{'Ops/s':<10} {'ECE':<7} {'Brier':<7} {'PROV-O↑':<8} \"\n",
    "          f\"{'Bridge(ms)':<11} {'nodes/s':<10}\")\n",
    "    print(\"-\" * 130)\n",
    "    for r in comparison_rows:\n",
    "        print(f\"{r['cpu']:<35} {r['cores']:<6} {r['ram_gb']:<6} {r['fusion_us']:<11} \"\n",
    "              f\"{r['fusion_ops']:<10} {r['ECE']:<7} {r['brier']:<7} {r['prov_speedup_avg']:<8} \"\n",
    "              f\"{r['bridge_1k_ms']:<11} {r['bridge_1k_nps']:<10}\")\n",
    "\n",
    "    # Verify invariants match across systems\n",
    "    eces = [r['ECE'] for r in comparison_rows if isinstance(r['ECE'], float)]\n",
    "    if len(eces) >= 2:\n",
    "        if all(abs(e - eces[0]) < 0.001 for e in eces):\n",
    "            print(f\"\\n✓ ECE invariant holds across all {len(eces)} systems\")\n",
    "        else:\n",
    "            print(f\"\\n✗ ECE varies across systems — investigate version mismatch!\")\n",
    "else:\n",
    "    for rf in result_files:\n",
    "        print(f\"  {os.path.basename(rf)}\")\n",
    "    print(\"\\nNeed ≥2 result files for cross-system comparison.\")\n",
    "    print(\"Share this notebook with collaborators to collect more runs.\")"
   ]
  }
 ]
}
